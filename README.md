# Vulnerability scanner

This is the core module that handles the security intelligence.

## Setting up

You first need to have [Node.js](https://nodejs.org/en/) installed, version 17 or higher.

1. Cloning the repository

```bash
git clone git@github.com:AndroneASQW/vulnerability-scanner.git
```

2. Checking out into the repository
```bash
cd vulnerability-scanner/
```

3. Cloning git submodules

```bash
git submodule update --init --recursive
```

The code snippet above can also be used to update `newsfeel-maps` if you need to.

4. Installing dependencies

```bash
npm install
```

5. (OPTIONAL) Download and unarchive `chrome_profile`

This needs to be done only once, when you first run the extractor and/ or scheduler.

```bash
wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1X5amZXIpsziGB1l6_znQ1lw6M19nVb2_' -O chrome_profile.zip
unzip chrome_profile.zip
```

6. (OPTIONAL) If you want to run the Chromium in headless mode, you need to install xvfb:

```bash
apt-get install -y xvfb
```

## How to run

### Running the extractor

You can run an extraction for a single article by running `node run extract [url] [siteMap] ....`.
Here's are all the available options* for this script.

```
run extract [url] [siteMap]

Extract data using given url

Options:
      --version         Show version number                            [boolean]
      --url             Url to extract data from.                     [required]
      --map             Path to the map type used in extraction.      [required]
  -o, --output          Path to output directory          [default: "./output/"]
      --nf-service      Url to a NewsFeelService endpoint
      --log-file        Path to log file            [default: "./extractor.log"]
      --abp-path        Path to AdblockPlus instance
                          [default: "./node_modules/newsfeel-browser/abp-3.12/"]
      --chrome-profile  Path to a chrome profile   [default: "./chrome_profile"]
      --gcloud          Whether this service is running in Google Cloud or not
  -h                    Show help                                      [boolean]
```
*`nf-service` is currently not used, the data will only be written to disk at the current stage.

For example, if you want to extract [this article](https://www.g4media.ro/breaking-presedintele-iohannis-viziteaza-sambata-tabara-mobila-pentru-refugiatii-ucraineni-de-la-siret.html) from [g4media](https://www.g4media.ro/), you can run the following command: 

```
node run extract --url https://www.g4media.ro/breaking-presedintele-iohannis-viziteaza-sambata-tabara-mobila-pentru-refugiatii-ucraineni-de-la-siret.html --map ./newsfeel-maps/selectors/g4media.yaml
```

### Running the scheduler

Very similar to the `extract` command, the scheduler makes use of `follow-links-extract` command.

```
run follow-links-extract [url] [siteMap] [propsFile]

Extract data from followed links

Options:
      --version         Show version number                            [boolean]
      --url             Url to extract data from.                     [required]
      --map             Path to the map type used in extraction.      [required]
  -p, --props           Path to property file.
                                             [default: "./extractor.properties"]
      --log-file        Path to log file            [default: "./extractor.log"]
      --abp-path        Path to AdblockPlus instance
                          [default: "./node_modules/newsfeel-browser/abp-3.12/"]
      --chrome-profile  Path to a chrome profile   [default: "./chrome_profile"]
      --gcloud          Whether this service is running in Google Cloud or not
  -h                    Show help                                      [boolean]
```

```bash
node run follow-links-extract --url https://www.g4media.ro/breaking-presedintele-iohannis-viziteaza-sambata-tabara-mobila-pentru-refugiatii-ucraineni-de-la-siret.html --map ./newsfeel-maps/selectors/g4media.yaml --props ./extractor.properties
```

The `scheduler.seedpage.base.selector` property, present in the `extractor.properties`
file must be modified accordingly to the root page element we want to focus on, in
terms of links following and extraction.

The extraction results are pushed to Mongo Atlas. To search through the extracted data
visit [MongoDB Atlas](https://www.mongodb.com/cloud/atlas/lp/try2?utm_source=google&utm_campaign=gs_emea_romania_search_core_brand_atlas_desktop&utm_term=mongodb%20atlas&utm_medium=cpc_paid_search&utm_ad=e&utm_ad_campaign_id=12212624554&adgroup=115749711343&gclid=CjwKCAjwi6WSBhA-EiwA6NiokximVV1bX3PNer76TSqnQXwDdaFhHHbAuNsOO0vKv97leNAMScX-uRoCuS4QAvD_BwE) and `Signup/Login` with dataworks.baby org gmail address.

Onced logged in, go to `Database Deployments` -> `Master` -> `Collections`, then
select the `News` collection and search through its articles.

## Development

### Updating modules

Once you already have the module cloned, you need to update either npm
packages, or `newsfeel-maps`. Here's how to do that.

#### Updating npm packages

It happens from time to time to make changes into `newsfeel-browser` or other package that need to be reflected here, as well. It's best to update
all packages, so just run:

```bash
npm update
```

#### Updating `newsfeel-maps`

Just run the snippet below:

```bash
git submodule update --init --recursive
```

### Testing

It's encouraged that you write tests for your code and make sure they pass before pushing to the remote repo.

For testing, we use [jest](https://jestjs.io/docs/getting-started) and you can run the tests through this snippet:

```bash
npm test
```
