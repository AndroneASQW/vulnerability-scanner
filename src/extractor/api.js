const diskIO = require('../utils/diskIO');
const { SourceOptions } = require('./options');
const logger = require('../utils/logger');
const { cleanupUrl } = require('../utils/stringManipulation');

/**
 * Extract from a loaded NewsFeelPage.
 * 
 * @param {enhancedBrowser.BrowserPage} page - The page we need to 
 * extract data from.
 * @param {SourceOptions} siteMap - The parse options that should be 
 * used in this case.
 */
async function extractFromBrowserPage(page, siteMap, url) {
  // TODO AA DELETE
  const parsedData = {};

  for (let parseOptions of siteMap.parseOptions) {
      if (parseOptions.itemName == 'comments')
          // For now, this is not supported, but it still
          // can be found in some site maps, so we just
          // skip over this if we find it.
          continue;
      
      const extractionFunction = parseOptions.getExtractionFunction();
      const result = await extractionFunction(page, parseOptions);
      parsedData[parseOptions.itemName] = result;
  }

  return parsedData;
}


/**
 * Method to extract all links from a from web page.
 * Used for checking each form from that web page
 */
async function extractLinksFromPage(page){
    const links = await page.page.$$eval('a', 
      (elements => elements.map(element => {
        const el = element;

        return {
          link: el.href
        };
      }))
    );
    return links;
}


/**
 * Method to populate input fields with malicious payload.
 */
async function populateInputField(page, inputFieldsId){
  
    var maliciousPayload = 'text';
    
    for (let inputFieldId of inputFieldsId){
        var inputField = await page.page.$$(`input[type="text"], [id="${inputFieldId.id}"], [name="${inputFieldId.name}"], [class="${inputFieldId.className}"]`);
        
        // TODO AA populate input field with malicious code not text
        for (var input of inputField) {
          await input.type('text');
        }
    }
    
    logger.info(`Submiting the malicious payload ${maliciousPayload} on input fields.`)
    await Promise.all([
      page.page.waitForNavigation(),
      page.page.$eval('form', form => form.submit())
    ]);
      
    return inputField;
}

/**
 * Method to extract the user input fields from a page. 
 */
async function getInputFieldsIdentificator(page) {
  var inputFields = await page.page.$$eval('input', inputs => {
    return inputs.map(input => {
      const id = input.getAttribute('id');
      const name = input.getAttribute('name');
      const className = input.getAttribute('class');

      return {
        id,
        name,
        className
      };
    });
  });
  return inputFields;
}


/**
 * Method used to pause the execution of the extraction process. 
 * Debugging purposes only
 */
async function pauseForUserInput() {
  const rl = readline.createInterface({
    input: process.stdin,
    output: process.stdout
  });

  return new Promise((resolve) => {
    rl.question('Press Enter to continue...', () => {
      rl.close();
      resolve();
    });
  });
}


/**
 * Run a full extraction based on CLI arguments.
 * 
 * @param {enhancedBrowser.enhancedBrowser} Browser Browser used in extraction.
 * @param {Object} args Parsed CLI argulemnts.
 * 
 */
async function runExtractionWithArguments(Browser, args) {
    
    logger.info(`Loading page at ${args.url}.`);
    const page = await Browser.getBrowserPageFor(args.url);

    logger.info(`Extracting links from web page ${args.url}`);
    const links = await extractLinksFromPage(page,args.url);
    logger.info(`Found links: ${links}`);

    // TODO AA remove link array limit
    for (let link of links.slice(0,7))
    {
      await page.page.goto(link.link);
      logger.info(`Extracting list of input fields from ${link.link}`);
      const inputFieldsId = await getInputFieldsIdentificator(page);

      logger.info(`Found inputs: ${inputFieldsId}`);
      const inputFields = await populateInputField(page, inputFieldsId);

      await pauseForUserInput();
    }
    
    // result.rawUrl = args.url;
    // result.url = cleanupUrl(args.url);
    // result.extractionTimestamp = Date.now();
    // logger.info(result);
    // diskIO.writeNewsData(args.output, '', result);
}


/**
 * Run an extraction based on the links that can be followed from an initial
 * page with articles listings.
 *
 * @param {Object} args Parsed CLI argulemnts.
 *
 */
 async function runFollowLinksExtraction(scheduler, args) {
    await scheduler.extractAndSend(args.url, args.map);
    logger.info(`Extraction finished for url: ${args.url} against site map: ${args.map}.`);
}

module.exports = {
    extractFromBrowserPage,
    runExtractionWithArguments,
    runFollowLinksExtraction
}
