const diskIO = require('../utils/diskIO');
const { SourceOptions } = require('./options');
const logger = require('../utils/logger');
const { cleanupUrl } = require('../utils/stringManipulation');

/**
 * Extract from a loaded NewsFeelPage.
 * 
 * @param {enhancedBrowser.BrowserPage} page - The page we need to 
 * extract data from.
 * @param {SourceOptions} siteMap - The parse options that should be 
 * used in this case.
 */

async function extractLinksFromPage(page, url){

    /**
     * Method to extract needed info from web page, such as links and
     * input data.
     */
    const links = await page.page.$$eval('a', (elements) => elements.map(el => el.href));

    return links;
}

async function extractInputFieldsFromPage(page){
    /**
     *  Method to extract the user input fields from a page.
     */

    const inputFields = await page.page.$$eval('input', inputs => {
    
    const fields = [];

    inputs.forEach(input => {
      const type = input.getAttribute('type');
      const name = input.getAttribute('name');
      const id = input.getAttribute('id');

      if (type !== 'hidden' && name && !fields.includes(name)) {
        fields.push(name);
      }

      if (type !== 'hidden' && id && !fields.includes(id)) {
        fields.push(id);
      }
    });

    return fields;
    });

  console.log('Input fields:', inputFields);
  return inputFields;

}

async function extractFromBrowserPage(page, siteMap, url) {

    const parsedData = {};

    for (let parseOptions of siteMap.parseOptions) {
        if (parseOptions.itemName == 'comments')
            // For now, this is not supported, but it still
            // can be found in some site maps, so we just
            // skip over this if we find it.
            continue;
        
        const extractionFunction = parseOptions.getExtractionFunction();
        const result = await extractionFunction(page, parseOptions);
        parsedData[parseOptions.itemName] = result;
    }

    logger.info(`Extracting links from web page ${url}`);
    const links = await extractLinksFromPage(page,url);
    logger.info(`Found links: ${links}`);

    logger.info(`Extracting input forms from ${url}`);
    const inputFields = await extractInputFieldsFromPage(page);
    logger.info(`Found inputs: ${inputFields}`);


    return parsedData;
}

/**
 * Run a full extraction based on CLI arguments.
 * 
 * @param {enhancedBrowser.enhancedBrowser} Browser Browser used in extraction.
 * @param {Object} args Parsed CLI argulemnts.
 * 
 */
async function runExtractionWithArguments(Browser, args) {

    //TODO AA remove Parsing the YAML map. Not needed in future.
    const siteMap = await SourceOptions.fromYAML(args.map);
    logger.info(`Loaded source map from ${args.map}`);
    
    const page = await Browser.getBrowserPageFor(args.url);
    logger.info(`Loaded page at ${args.url}.`);

    //TODO AA remove this later
    const result = await extractFromBrowserPage(page, siteMap,args.url);

    result.rawUrl = args.url;
    result.url = cleanupUrl(args.url);
    result.extractionTimestamp = Date.now();
    logger.info(result);
    diskIO.writeNewsData(args.output, '', result);
}


/**
 * Run an extraction based on the links that can be followed from an initial
 * page with articles listings.
 *
 * @param {Object} args Parsed CLI argulemnts.
 *
 */
 async function runFollowLinksExtraction(scheduler, args) {
    await scheduler.extractAndSend(args.url, args.map);
    logger.info(`Extraction finished for url: ${args.url} against site map: ${args.map}.`);
}

module.exports = {
    extractFromBrowserPage,
    runExtractionWithArguments,
    runFollowLinksExtraction
}
